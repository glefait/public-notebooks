{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165dd188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kudos to https://nodes.guru/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392fccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_download = False\n",
    "force_download_on_build_id_change = True\n",
    "RELATIVE_LOCAL_PATH = os.path.join('data', 'nodes_guru')\n",
    "\n",
    "ALL_PROJECTS_FILE = \"nodes-guru-all-projects.json\"\n",
    "NODESGURU_BUILD_ID_VERSION=\"nodes-guru.buildid\"\n",
    "FINAL_CSV=\"nodes-guru-project-summary.csv\"\n",
    "os.makedirs(RELATIVE_LOCAL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880db620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the projects\n",
    "all_projects_filepath = os.path.join(RELATIVE_LOCAL_PATH, ALL_PROJECTS_FILE)\n",
    "if not force_download and force_download_on_build_id_change:\n",
    "    # check the current page to get the buildId\n",
    "    page = requests.get('https://nodes.guru')\n",
    "    m = re.search('\"buildId\":\"([^\"]+)\"', page.text)\n",
    "    if m:\n",
    "        current_build_id=m.group(1)\n",
    "    else:\n",
    "        print(\"buildID not found ...\")\n",
    "        current_build_id=\"HNqu_x6wipeTmVq9jcFp1\"\n",
    "    local_build_id = None\n",
    "    if os.path.exists(os.path.join(RELATIVE_LOCAL_PATH, NODESGURU_BUILD_ID_VERSION)):\n",
    "        with open(os.path.join(RELATIVE_LOCAL_PATH, NODESGURU_BUILD_ID_VERSION)) as r:\n",
    "            local_build_id = r.read()\n",
    "    if local_build_id == current_build_id:\n",
    "        # downloaded with same buildId\n",
    "        force_download_on_build_id_change = False\n",
    "    else:\n",
    "        with open(os.path.join(RELATIVE_LOCAL_PATH, NODESGURU_BUILD_ID_VERSION), 'w') as w:\n",
    "            w.write(current_build_id)\n",
    "\n",
    "if not os.path.exists(all_projects_filepath) or force_download or force_download_on_build_id_change:\n",
    "    r = requests.get('https://nodes.guru/api/search')\n",
    "    with open(all_projects_filepath, 'w') as w:\n",
    "        w.write(json.dumps(r.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the projects\n",
    "with open(all_projects_filepath) as r:\n",
    "    data = json.loads(r.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b3800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the project data\n",
    "for project in data:\n",
    "    project_slug = project['slug']\n",
    "    path = os.path.join(RELATIVE_LOCAL_PATH, f'{project_slug}.json')\n",
    "    if not os.path.exists(path) or force_download or force_download_on_build_id_change:\n",
    "        project_data = requests.get(f'https://nodes.guru/_next/data/HNqu_x6wipeTmVq9jcFp1/{project_slug}.json')\n",
    "        with open(path, 'w') as w:\n",
    "            w.write(json.dumps(project_data.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d3c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the project data. Let's select some data\n",
    "projects_data = []\n",
    "for project in data:\n",
    "    project_slug = project['slug']\n",
    "    path = os.path.join(RELATIVE_LOCAL_PATH, f'{project_slug}.json')\n",
    "    with open(path, 'r') as r:\n",
    "        project_data = json.loads(r.read())\n",
    "        startDate = project_data['pageProps']['data']['additionalInfo']['startDate']\n",
    "        if startDate is not None:\n",
    "            startDate = str(datetime.datetime.fromtimestamp(int(startDate.split(';')[1].strip())).date())\n",
    "        \n",
    "        web = project_data['pageProps']['data']['socialLinks']['website']\n",
    "        twitter = project_data['pageProps']['data']['socialLinks']['twitter']\n",
    "        if twitter:\n",
    "            twitter = twitter.split('?')[0].lstrip('https://twitter.com/')\n",
    "        projects_data.append([project_slug, web, twitter, startDate, project_data['pageProps']['data']['additionalInfo']['rewards']])\n",
    "\n",
    "# generate the pandas DataFrame\n",
    "df = pd.DataFrame(data=projects_data, columns=['project', 'web', 'twitter', 'rewards_start_date', 'rewards_details']).sort_values('project')\n",
    "df.to_csv(os.path.join(RELATIVE_LOCAL_PATH, FINAL_CSV), index=False)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
